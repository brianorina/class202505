


import pandas as pd
import numpy as np

from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsRegressor
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import matplotlib.pyplot as plt
import seaborn as sns





url = "https://s3-api.us-geo.objectstorage.softlayer.net/cf-courses-data/CognitiveClass/ML0101ENv3/labs/FuelConsumptionCo2.csv"
data = pd.read_csv(url)

# Preview the first few rows
print(data.head())





# Show data info, summary statistics, and check for missing values
print(data.info())
print(data.describe())
print("Missing values:\n", data.isnull().sum())

#Visualize target distribution
plt.figure(figsize=(6,4))
sns.histplot(data['CO2EMISSIONS'], kde=True)
plt.title('Distribution of CO2 Emissions')
plt.xlabel('CO2 Emissions (g/km)')
plt.show()





# For this exercise, let's use numerical features only for KNN (KNN does not handle categoricals without encoding)
features = ['ENGINESIZE', 'CYLINDERS', 'FUELCONSUMPTION_COMB']
target = 'CO2EMISSIONS'

X = data[features]
y = data[target]

# Train-test split (80-20)
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Scale features (KNN is distance-based, so scaling is important)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)





# Instantiate KNN regressor (default k=5)
model_knn = KNeighborsRegressor(n_neighbors=5)
model_knn.fit(X_train_scaled, y_train)


# Instantiate Linear Regression
model_linreg = LinearRegression()
model_linreg.fit(X_train_scaled, y_train)





def mean_absolute_percentage_error(y_true, y_pred):  # scikit-learn < 1.1 doesn't have this built-in
    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100





# KNN metrics
knn_pred = model_knn.predict(X_test_scaled)
knn_mae = mean_absolute_error(y_test, knn_pred)
knn_mse = mean_squared_error(y_test, knn_pred)
knn_rmse = mean_squared_error(y_test, knn_pred, squared=False)
knn_r2 = r2_score(y_test, knn_pred)
knn_mape = mean_absolute_percentage_error(y_test, knn_pred)





# Linear Regression metrics
linreg_pred = model_linreg.predict(X_test_scaled)
linreg_mae = mean_absolute_error(y_test, linreg_pred)
linreg_mse = mean_squared_error(y_test, linreg_pred)
linreg_rmse = mean_squared_error(y_test, linreg_pred, squared=False)
linreg_r2 = r2_score(y_test, linreg_pred)
linreg_mape = mean_absolute_percentage_error(y_test, linreg_pred)


# Compare in a DataFrame for clarity
results = pd.DataFrame({
    'Model': ['KNN', 'Linear Regression'],
    'MAE': [knn_mae, linreg_mae],
    'MSE': [knn_mse, linreg_mse],
    'RMSE': [knn_rmse, linreg_rmse],
    'R2': [knn_r2, linreg_r2],
    'MAPE': [knn_mape, linreg_mape]
})

print(results)





# INTERPRETATION: WHICH MODEL IS BETTER?

print("Comparison Table:\n", results)

best_model = results.loc[results['RMSE'].idxmin(), 'Model']
print(f"\nModel with lowest RMSE: {best_model}")

# Quick explanation for the user
print(
    "\nHow to interpret:\n"
    "- Lower MAE, MSE, RMSE, and MAPE = better (closer to 0).\n"
    "- Higher R² (closer to 1) = better model fit.\n"
    "- In practice, focus on RMSE and R² for regression.\n"
    "- Double-check for negative R² or very high errors (could signal data issues or poor fit).\n"
)

# Optional: Give a plain language summary
if knn_rmse < linreg_rmse:
    print("KNN performed better in terms of RMSE (closer predictions).")
elif linreg_rmse < knn_rmse:
    print("Linear Regression performed better in terms of RMSE.")
else:
    print("Both models performed similarly.")



